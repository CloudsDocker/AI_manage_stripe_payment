{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1kVZyOMXHq_rbOetq9B0CjiEeBdJfrGbQ",
      "authorship_tag": "ABX9TyMkWe+ZdifcPvni+Q46j/Nc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CloudsDocker/AI_manage_stripe_payment/blob/main/seq2seq_lei_zhang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run below command to install libraries"
      ],
      "metadata": {
        "id": "OF5M-AChlvCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQsvtKjVXxGZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mound and load data files ( to mount the files needed)"
      ],
      "metadata": {
        "id": "avCznvy5l2Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Path to the zip file containing training data\n",
        "data_file_path = \"drive/MyDrive/Cooking_Dataset\"\n",
        "data_file_path = '/content/drive/MyDrive/Cooking_Dataset'  # Google drive\n",
        "\n",
        "train_data_path = os.path.join(data_file_path, \"train\")\n",
        "\n",
        "print(os.path.exists(train_data_path))\n",
        "MAX_FILES = 10"
      ],
      "metadata": {
        "id": "iWPMuAaM3Bay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89887ae-8ba0-423c-9f34-9f12320f43b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Define the self-attention mechanism\n",
        "# class SelfAttention(nn.Module):\n",
        "#     def __init__(self, hidden_size):\n",
        "#         super(SelfAttention, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "\n",
        "#         # Define layers for Q, K, and V projections\n",
        "#         self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
        "#         self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
        "#         self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "#     def forward(self, query, key, value):\n",
        "#         # Project input tensors\n",
        "#         Q = self.linear_q(query)\n",
        "#         K = self.linear_k(key)\n",
        "#         V = self.linear_v(value)\n",
        "\n",
        "#         # Compute attention scores\n",
        "#         scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.hidden_size).float())\n",
        "\n",
        "#         # Apply softmax to obtain attention weights\n",
        "#         attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "#         # Compute attention output\n",
        "#         attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "#         return attention_output\n",
        "\n",
        "# Define the Seq2Seq model with self-attention\n",
        "# class Seq2Seq_WithAttn(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size):\n",
        "#         super(Seq2Seq, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#\n",
        "#         # Define encoder layers\n",
        "#         self.encoder_embedding = nn.Embedding(input_size, hidden_size)\n",
        "#         self.encoder_self_attention = SelfAttention(hidden_size)\n",
        "#\n",
        "#         # Define decoder layers\n",
        "#         self.decoder_embedding = nn.Embedding(output_size, hidden_size)\n",
        "#         self.decoder_self_attention = SelfAttention(hidden_size)\n",
        "#\n",
        "#         # Define fully connected layer to output vocabulary\n",
        "#         self.fc = nn.Linear(hidden_size, output_size)\n",
        "#\n",
        "#     def forward(self, input_seq, target_seq):\n",
        "#         # Encoder forward pass\n",
        "#         encoder_embedded = self.encoder_embedding(input_seq)\n",
        "#         encoder_output = self.encoder_self_attention(encoder_embedded, encoder_embedded, encoder_embedded)\n",
        "#\n",
        "#         # Decoder forward pass\n",
        "#         decoder_embedded = self.decoder_embedding(target_seq)\n",
        "#         decoder_output = self.decoder_self_attention(decoder_embedded, decoder_embedded, decoder_embedded)\n",
        "#\n",
        "#         # Apply fully connected layer to decoder output\n",
        "#         output = self.fc(decoder_output)\n",
        "#\n",
        "#         return output\n",
        "\n",
        "from typing import List, Tuple\n",
        "def preprocess_data(ingredients: List[str], instructions: List[str]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Preprocesses the recipe data by tokenizing and converting to tensors.\n",
        "    Returns lists of ingredient and instruction tensors.\n",
        "    \"\"\"\n",
        "    # tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    # Extract just the ingredient names, ignoring the numbers and units\n",
        "    ingredient_names = []\n",
        "    for ingr_text in ingredients:\n",
        "        ingr_parts = ingr_text.split(',')\n",
        "        ingr_names = [re.sub(r'\\d+\\s*\\w*', '', part.strip()) for part in ingr_parts]\n",
        "        ingredient_names.append(' '.join(ingr_names))\n",
        "\n",
        "\n",
        "\n",
        "def load_data_from_all_files(data_dir, max_files=None):\n",
        "    ingredients = []\n",
        "    instructions = []\n",
        "\n",
        "    for file_name in os.listdir(data_dir):\n",
        "        if max_files and len(ingredients) >= max_files:\n",
        "            break\n",
        "        with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n",
        "            print(f\"Reading file: {file_name}\")\n",
        "            recipe_parts = file.read().split('END RECIPE\\n')\n",
        "            for recipe in recipe_parts:\n",
        "                lines = recipe.split('\\n')\n",
        "                ingr_line = next((line for line in lines if line.startswith('ingredients:')), None)\n",
        "                if ingr_line:\n",
        "                    ingr_text = ingr_line.split(':')[1].strip()\n",
        "                    # ingr_items = ingr_line.split(':')[1].strip().split(\"\\t\")\n",
        "                    # ingr_text = ' '.join(ingr_items)\n",
        "\n",
        "                    # to get the instructions from following lines until the end of the recipe\n",
        "                    instr_start = lines.index(ingr_line)\n",
        "                    instr_text = '. '.join(line.strip() for line in lines[instr_start+1:])\n",
        "\n",
        "                    if ingr_text and instr_text:\n",
        "                        ingredients.append(ingr_text)\n",
        "                        instructions.append(instr_text)\n",
        "\n",
        "    return ingredients, instructions\n",
        "\n",
        "# Function to load data from zip file\n",
        "# def load_data_from_zip(zip_file_path):\n",
        "#     data = []\n",
        "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#         for file_info in zip_ref.infolist():\n",
        "#             with zip_ref.open(file_info) as file:\n",
        "#                 # Assuming each file contains input-output pairs separated by newline\n",
        "#                 pairs = file.read().decode('utf-8').strip().split('\\n')\n",
        "#                 data.extend([pair.split('\\t') for pair in pairs])\n",
        "#     return data\n",
        "# Function to convert text to tensor\n",
        "def text_to_tensor(text, vocab):\n",
        "    tensor = [vocab[token] for token in text.split()]\n",
        "    return torch.tensor(tensor, dtype=torch.long).to(device)\n",
        "\n",
        "# Data preprocessing\n",
        "# tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer):\n",
        "    for text in data_iter:\n",
        "        if isinstance(text, list):\n",
        "            yield tokenizer(' '.join(text))\n",
        "        else:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "\n",
        "\n",
        "# Load training data from the zip file\n",
        "ingredients, instructions = load_data_from_all_files(train_data_path, MAX_FILES) # total list of ingredients and instructions\n",
        "\n",
        "\n",
        "def preprocess_data(ingredients: List[str], instructions: List[str]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Preprocesses the recipe data by tokenizing and converting to tensors.\n",
        "    Returns lists of ingredient and instruction tensors.\n",
        "    \"\"\"\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    # Normalize ingredient text\n",
        "    normalized_ingredients = []\n",
        "    for ingr_text in ingredients:\n",
        "        ingr_parts = re.split(r'[;,]', ingr_text)\n",
        "        ingr_names = [re.sub(r'\\d+\\s*\\w*', '', part.strip()) for part in ingr_parts]\n",
        "        normalized_ingredient = ' '.join(ingr_names)\n",
        "        normalized_ingredients.append(normalized_ingredient)\n",
        "\n",
        "    vocab_ingr = build_vocab_from_iterator(yield_tokens(normalized_ingredients, tokenizer), specials=['<unk>', '<pad>'])\n",
        "    vocab_instr = build_vocab_from_iterator(yield_tokens(instructions, tokenizer), specials=['<unk>', '<pad>'])\n",
        "\n",
        "    ingr_tensors = [torch.tensor(vocab_ingr.lookup_indices(tokenizer(ingr)), dtype=torch.long, device=device) for ingr in normalized_ingredients]\n",
        "    instr_tensors = [torch.tensor(vocab_instr.lookup_indices(tokenizer(instr)), dtype=torch.long, device=device) for instr in instructions]\n",
        "\n",
        "    return ingr_tensors, instr_tensors, vocab_ingr, vocab_instr # the tensor representation of ingredients and instructions, same length as the input\n",
        "\n",
        "\n",
        "#TODO: to split by tab and space then build vocab\n",
        "# to load data\n",
        "# vocab_ingr = build_vocab_from_iterator(yield_tokens(ingredients), specials=['<unk>'])\n",
        "# vocab_instr = build_vocab_from_iterator(yield_tokens(instructions), specials=['<unk>'])\n",
        "#\n",
        "# # def build_vocab_by_tokenizer(data, tokenizer, specials=None):\n",
        "#\n",
        "#\n",
        "# train_data = [vocab_ingr(ingr) for ingr in ingredients]\n",
        "# train_data = [vocab_instr(instr) for instr in instructions]\n",
        "#\n",
        "# # Create datasets and data loaders\n",
        "# train_dataset = to_map_style_dataset(list(zip(train_data, train_data)))\n",
        "#\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "ingr_tensors, instr_tensors, vocab_ingr, vocab_inst = preprocess_data(ingredients, instructions)\n",
        "# Now you can use ingr_tensors and instr_tensors to train your neural network\n",
        "# For example, you can create a PyTorch dataset and dataloader:\n",
        "# train_dataset = torch.utils.data.TensorDataset(ingr_tensors, instr_tensors)\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "# Model architecture\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size,  dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # print(\"src_lengths:\", src_lengths)\n",
        "        # print(\"Input shape: %s and type: %s and data: %s\" % (src.shape, src.type(), src))\n",
        "        # Add a batch dimension to the input tensor if it's 1-dimensional\n",
        "        # if len(src.shape) == 1:\n",
        "        #     print(\"Shape is 1-dimensional\")\n",
        "        #     src = src.unsqueeze(0)  # Assuming src is a 1-dimensional tensor\n",
        "        # else:\n",
        "        #     print(\"Shape is not 1-dimensional, the dimension is:\", len(src.shape))\n",
        "        # print(\"After unsqueeze shape:\", src.shape)\n",
        "\n",
        "        # if src.is_cuda:\n",
        "        #   print(\"src is on GPU\")\n",
        "        # else:\n",
        "        #   print(\"src is on CPU\")\n",
        "        # print(f\"embedding layer device: {self.embedding.weight.device}\")\n",
        "        # print(f\"dropout layer device: {self.dropout.device}\")  # Assuming 'dropout' is a layer\n",
        "\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded = embedded.unsqueeze(0)\n",
        "        # print(\"Embedded shape:\", embedded.shape)\n",
        "        # packed_embedded = rnn_utils.pack_padded_sequence(embedded, src_lengths, enforce_sorted=False)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, emb_dim, hidden_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = input_size\n",
        "        self.hid_dim = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_size, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_size,1,  dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_size, input_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # Add batch dimension to input\n",
        "        # input = input.unsqueeze(0)\n",
        "\n",
        "        # Add batch dimension to hidden and cell states\n",
        "        # hidden = hidden.unsqueeze(0)\n",
        "        # cell = cell.unsqueeze(0)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        # embedded = embedded.unsqueeze(0)\n",
        "        # print(\"shape of embedded:\", embedded.shape)\n",
        "        # print(\"shape of hidden:\", hidden.shape)\n",
        "        # print(\"shape of cell:\", cell.shape)\n",
        "        # Add batch dimension to hidden and cell states\n",
        "        # Add batch dimension and sequence dimension to hidden and cell states\n",
        "        # hidden = hidden.unsqueeze(0).unsqueeze(0)  # (1, 1, 256)\n",
        "        # cell = cell.unsqueeze(0).unsqueeze(0)  # (1, 1, 256)\n",
        "        # print(\"shape of embedded:\", embedded.shape)\n",
        "        # print(\"shape of hidden:\", hidden.shape)\n",
        "        # print(\"shape of cell:\", cell.shape)\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # print(\"shape of output:\", output.shape)\n",
        "        # print((\"value of output:\", output))\n",
        "        # Remove batch dimension from output\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction = self.softmax(self.fc_out(output[0]))\n",
        "        # print(\"shape of prediction:\", prediction.shape)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "# class Seq2Seq(nn.Module):\n",
        "#     def __init__(self, encoder, decoder, device):\n",
        "#         super().__init__()\n",
        "#         self.encoder = encoder\n",
        "#         self.decoder = decoder\n",
        "#         self.device = device\n",
        "#\n",
        "#     def forward(self, src, trg, teacher_forcing_ratio=TEACHER_FORCING_RATIO):\n",
        "#         batch_size = src.shape[1]\n",
        "#         trg_len = trg.shape[0]\n",
        "#         trg_vocab_size = self.decoder.output_dim\n",
        "#\n",
        "#         outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "#         hidden, cell = self.encoder(src)\n",
        "#\n",
        "#         input = trg[0, :]\n",
        "#         for t in range(1, trg_len):\n",
        "#             output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "#             outputs[t] = output\n",
        "#             teacher_force = random.random() < teacher_forcing_ratio\n",
        "#             top1 = output.argmax(1)\n",
        "#             input = trg[t] if teacher_force else top1\n",
        "#\n",
        "#         return outputs\n",
        "def checkDevice(data, text):\n",
        "  if data.is_cuda:\n",
        "    print(f\"{text} is on GPU\")\n",
        "  else:\n",
        "    print(f\"{text} is on CPU\")\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show()\n",
        "\n",
        "# Hyperparameters\n",
        "INPUT_DIM = len(ingr_tensors)\n",
        "OUTPUT_DIM = len(instr_tensors)\n",
        "print(\"ingr_tensors length:\", len(ingr_tensors))\n",
        "print(\"instr_tensors length:\", len(instr_tensors))\n",
        "# checkDevice(ingr_tensors[0],\"ingr_tensors\")\n",
        "# checkDevice(ingr_tensors[0],\"instr_tensors\")\n",
        "# ingr_tensors length: 3289\n",
        "# instr_tensors length: 3289\n",
        "ENC_EMB_DIM = len(vocab_ingr)\n",
        "DEC_EMB_DIM = len(vocab_inst)\n",
        "print(\"ENC_EMB_DIM:\", ENC_EMB_DIM)\n",
        "print(\"DEC_EMB_DIM:\", DEC_EMB_DIM)\n",
        "\n",
        "hidden_size = 256\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "MAX_LENGTH = 100\n",
        "LEARNING_RATE = 0.01\n",
        "TEACHER_FORCING_RATIO = 1.0\n",
        "\n",
        "\n",
        "# ========  declare variables ========\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "# Instantiate models\n",
        "enc = Encoder( ENC_EMB_DIM, 128, hidden_size, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = Decoder( DEC_EMB_DIM, 128,hidden_size, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "\n",
        "# model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Define optimization and loss function\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.NLLLoss()\n",
        "\n",
        "# Training loop\n",
        "# N_EPOCHS = 10\n",
        "\n",
        "encoder_optimizer = optim.Adam(enc.parameters(), lr=LEARNING_RATE)\n",
        "decoder_optimizer = optim.Adam(dec.parameters(), lr=LEARNING_RATE)\n",
        "loss_total_print = 0\n",
        "loss_total_plot = 0\n",
        "plot_losses = []\n",
        "print_every = 10\n",
        "plot_every = 30\n",
        "\n",
        "for epoch in range(MAX_LENGTH):\n",
        "    train_loss = 0\n",
        "    start_time = time.time()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # to get a random index of INPUT_DIM\n",
        "    random_data_idx = random.randint(0, INPUT_DIM)\n",
        "    input_tensor = ingr_tensors[random_data_idx]\n",
        "    target_tensor = instr_tensors[random_data_idx]\n",
        "\n",
        "    # Calculate the lengths of the sequences in input_tensor\n",
        "    # src_lengths = [len(seq) for seq in input_tensor]\n",
        "\n",
        "    # Create a tensor of sequence lengths\n",
        "    # src_lengths_tensor = torch.tensor(src_lengths).to(device)\n",
        "\n",
        "    # ==== encoder loop ====\n",
        "    # loop of each input tensor in encoder\n",
        "    # Iterate over each sequence in input_tensor\n",
        "    for src in input_tensor:\n",
        "        # Ensure src has at least one dimension\n",
        "        if src.dim() == 0:\n",
        "            src = src.unsqueeze(0)\n",
        "\n",
        "        # Get the length of the sequence\n",
        "        seq_length = src.size(0)\n",
        "\n",
        "        # print(f\"src device: {src.device}\")\n",
        "        # print(f\"embedding layer device: {Encoder.embedding.weight.device}\")  # Assuming 'embedding' is a layer\n",
        "        # print(f\"dropout layer device: {Encoder.dropout.weight.device}\")  # Assuming 'dropout' is a layer\n",
        "\n",
        "        # Pass src and its length to the encoder\n",
        "        hidden, cell = enc(src.unsqueeze(0), torch.tensor([seq_length]).to(device))\n",
        "\n",
        "\n",
        "\n",
        "        # hidden = hidden.squeeze(0)\n",
        "        # cell = cell.squeeze(0)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)  # <SOS> token\n",
        "    # decoder_hidden = hidden\n",
        "\n",
        "    # ====== decoder loop ======\n",
        "    # print(\"target_tensor shape:\", target_tensor.shape)\n",
        "    # print(\"target value at index idx:\", target_tensor[random_data_idx])\n",
        "    target_length = len(target_tensor)\n",
        "    # loop of each target tensor in decoder\n",
        "    # target_tensor = target_tensor.unsqueeze(0).to(device)  # Reshape target for batch size 1\n",
        "\n",
        "    # Reshape target for teacher forcing\n",
        "    target_tensor = target_tensor.unsqueeze(0)  # Batch dimension\n",
        "    target_tensor = torch.nn.functional.pad(target_tensor, (1, 0), value=EOS_token)  # Pad with EOS token\n",
        "\n",
        "    for i in range(target_length):\n",
        "        # trg = target_tensor[i]\n",
        "        # trg=trg.unsqueeze(0)\n",
        "        # print(\"i:\", i)\n",
        "        trg = target_tensor[:, i]  # Access current element within the batch\n",
        "\n",
        "        # trg = target_tensor[:, i]  # Access current element within the batch\n",
        "\n",
        "        output, hidden, cell = dec(decoder_input, hidden, cell)\n",
        "        # print(\"output shape:\", output.shape)\n",
        "        # print(\"target_tensor shape:\", target_tensor.shape)\n",
        "        # print(\"trg shape:\", trg.shape)\n",
        "        # print(\"trg value:\", trg)\n",
        "        train_loss += criterion(output, trg)\n",
        "        # print(\"train_loss:\", train_loss)\n",
        "        # check if we need to use teacher forcing\n",
        "        teacher_force = random.random() < TEACHER_FORCING_RATIO\n",
        "        if teacher_force:\n",
        "            decoder_input = trg.unsqueeze(0)\n",
        "        else:\n",
        "            topv, topi = output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    train_loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    #\n",
        "    # # for batch in train_dataloader:\n",
        "    # #     src = batch[0].to(device)\n",
        "    # #     trg = batch[1].to(device)\n",
        "    #\n",
        "    # optimizer.zero_grad()\n",
        "    #\n",
        "    # output = model(src, trg)\n",
        "    #\n",
        "    # output_dim = output.shape[-1]\n",
        "    #\n",
        "    # output = output[1:].view(-1, output_dim)\n",
        "    # trg = trg[1:].reshape(-1)\n",
        "    #\n",
        "    # loss = criterion(output, trg)\n",
        "    #\n",
        "    # loss.backward()\n",
        "    #\n",
        "    # optimizer.step()\n",
        "    #\n",
        "    # train_loss += loss.item()\n",
        "    #\n",
        "    # train_loss /= len(train_dataloader)\n",
        "    loss_avg = train_loss.item() / target_length\n",
        "    loss_total_print += loss_avg\n",
        "    loss_total_plot += loss_avg\n",
        "    if epoch % print_every == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss_avg:.3f}\")\n",
        "        loss_total_print = 0\n",
        "    if epoch % plot_every == 0:\n",
        "        plot_loss_avg = loss_total_plot / plot_every\n",
        "        plot_losses.append(plot_loss_avg)\n",
        "        loss_total_plot = 0\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins = int((end_time - start_time) // 60)\n",
        "    epoch_secs = int((end_time - start_time) % 60)\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{MAX_LENGTH} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "\n",
        "showPlot(plot_losses)\n",
        "#\n",
        "# # Define input and output vocabulary sizes\n",
        "# input_size = 10  # Example: Size of input vocabulary\n",
        "# output_size = 10  # Example: Size of output vocabulary\n",
        "#\n",
        "# # Define hidden size\n",
        "# hidden_size = 256  # Example: Size of hidden layers\n",
        "#\n",
        "# # Initialize Seq2Seq model\n",
        "# model = Seq2Seq(input_size, hidden_size, output_size)\n",
        "#\n",
        "# # Define loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#\n",
        "#\n",
        "# # Example training loop (assuming input and target sequences are tensors)\n",
        "# num_epochs = 10  # Example: Number of epochs\n",
        "# for epoch in range(num_epochs):\n",
        "#     for input_text, target_text in training_data:\n",
        "#         # Convert input and target texts to tensors (you need to implement this)\n",
        "#         input_seq = text_to_tensor(input_text)\n",
        "#         target_seq = text_to_tensor(target_text)\n",
        "#\n",
        "#         optimizer.zero_grad()\n",
        "#\n",
        "#         # Forward pass\n",
        "#         output = model(input_seq, target_seq)\n",
        "#\n",
        "#         # Calculate loss\n",
        "#         loss = criterion(output.view(-1, output_size), target_seq.view(-1))\n",
        "#\n",
        "#         # Backpropagation\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#\n",
        "#     # Example: Print loss after each epoch\n",
        "#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "HMmJYqto2tq_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}